* ERAD RS 2019 - Leitura e resumo dos slides 
** Modelo PCAM

   Apresentação de conceitos para construção de programas paralelos:

   - Particionamento
   - Comunicação
   - Aglomeração
   - Mapeamento

   Etapas:
   - Particionamento / Comunicação: *Focam na escabilidade de uma
     solução*. 
     - Procurar algoritmos que oferecem um paralelismo extremo;
     - A maior concorrência possível entre as unidades de
       processamento; 

   - Aglomeração / Mapeamento: *Focam na preocupação com desempenho*. 
     - Necessita de conhecimentos de configuração da plataforma de
       execução.
     - Como mapear para os recursos computacionais.
     - Como balancear a carga.

   Provavelmente esse tipo de técnica pode ser aplicada de maneira
   cíclica.

*** Particionamento
    *Objetivo:* Descobrir oportunidades de paralelismo.
     - Identificar operações que podem ser utilizadas em paralelo.
     - Identificar a menor operação possível, a menor *granulidade
       (grão)*.

    Um tamanho muito pequeno para operação poderia causar problemas,
    como: comunicação excessiva e tarefas demais.

**** Particionamento de dados (decomposição de domínio)
       *Objetivo:* Particionamento do problema com enfoque em seus
        dados.
       Cada pedaço do problema terá os dados e suas operações,
        usualmente dividos num mesmo tamanho.

**** Particionamento de operações (decomposição funcional)
       *Objetivo:* Particionamento do problema com enfoque em suas
        operações.
       Divide-se as operações que podem ser executadas de maneira
        concorrente, criando *partes funcionais.*
*** Comunicação
    *Objetivo:* Fase essencial para que os processos conversem entre si,
     compartilhando informações; no entanto, a comunicação implica em
     sincronização, o que *prejudica o alto desempenho.*

**** Definições
       - *Local:* Necessita dados de poucas partições vizinhas.
       - *Global:* Necessita dados de todas as partições.
       - *Assíncrona :* O emissor não sabe quando os dados serão
         utilizados, apenas os envia.
       - *Síncrona :* Emissor e receptor estão cientes quando a operação
         acontece.
*** Aglomeração
    *Objetivo :* Tornar o algortimo mais realista, considerando os
     limites impostos pela plataforma alvo. E também, obter um
     programa eficiente na plataforma, aumentando a localidade do
     cálculo.
   *Benefícios :*
   - Impacto em diretivas de comunicação.
   - Benefícios da replicação de dados e operações.
   - Necessidade da redução drástica do número total de tarefas (uma
     por unidade de processamento).
*** Mapeamento
    *Objetivo :* Definir onde cada tarefa será executada. É uma tarefa
    complexa, pois: 
    - Deve ser explícito em supercomputadores de alto desempenho.
    - Soluções simples podem ser empregados; entretanto, com um baixo
      desempenho.

**** Mapeamento simples
     *Pré-condições :*
     - Custo computacional das tarefas é *estático* (ao longo do tempo)
     - Custo computacional das tarefas é *homogêneo* (entre as
       partições)
     - Supercomputador tem capidade *homogênea*, e a rede de
       interconexão também.

     *Aglomeramos as partições para ter uma por núcleo,* *assim
     evitamos* *ao máximo a comunicação.*
     
**** Mapeamento mais complexo
     Cenário onde as partições tem custos computacionais diferentes
     (*heterogêneas*), ainda que sejam estáticas ao longo do tempo de
     execução.

***** Algoritmos de balanceamento de carga
      Estes algoritmos permitem equilibrar o cálculo, e também
      encontrar a melhor granulidade.

****** Bisseção recursiva (*Barnes-Hut, centralizado/distribuído*)
       - *Simples :* unicamente pelas coordenadas.
       - *Melhor :* método de balanceamento (somente *comunicações*).
       - *Bisseção recursiva :* para malhas irregulares.

****** Balanceamento de carga local
       Algoritmo distribuído.
***** Escalonamento de tarefas 
      Existe um *maestro* escalonador que define em quais núcleos de
      processamento as tarefas irão executar.

      *Funcionamento :* /poll of problems/
      O maestro equipa-se de uma heurística de escalonamento, onde ele
      decide durante a execução onde mapear as tarefas.
     
* SIMGRID
** Instalação no Mac OSX

*** Instalação da biblioteca C *Boost*:

   #+begin_src shell
   brew install boost
   #+end_src

*** Download do SIMGRID no site oficial:
    
   [[https://simgrid.org][Site oficial]]

*** Extrair o arquivo:

   #+begin_src shell 
   tar -xvf SimGrid-x.x.x.tar.gz  
   #+end_src

*** Entrar no diretório:

   #+begin_src shell
   cd SimGrid-x.x.x
   #+end_src

*** Gerar todos os /makefiles/ (assumindo que você deseja instalar no /folder/ /usr/local):

   #+begin_src shell
   cmake -DCMAKE_INSTALL_PREFIX=/usr/local -Denable_smpi=on -Denable_documentation=off
   #+end_src

*** Alterar no arquivo *CmakeCache.txt* de */usr/bin/python* para */usr/bin/python3*
*** Compilar os arquivos:
  
   #+begin_src shell
   make
   #+end_src

*** Executar os testes:

   #+begin_src shell 
   make check
   #+end_src

*** Instalar bibliotecas e executáveis:

   #+begin_src shell
   sudo make install
   #+end_src

*** Versões para instalação

   *Cmake 3.15.1*
   *Python 3+*
** Descrevendo as plataformas de simulação
   Para utilização do *SMPI* é necessário descrever qual a topologia do
   ambiente, e para isso utiliza-se um arquivo XML contendo
   informações dos /hosts/ e dos /links/ que os interligam.

*** Três /hosts/

    Uma das topologias mais simples que foram apresentadas, contém 3
    /hosts/ conectados através de 3 /links/, onde existe um /host/ com muito
    poder computacional (host2), 40 vezes mais que o host0, e o link2
    possuindo 5 vezes mais velocidade que o link1.

    #+CAPTION: Figura que demonstra a topologia com 3 hosts interconectados.
    #+NAME: fig:3-HOSTS
    [[./img/3-host.png]]
    #+begin_src xml
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
<platform version="4.1">
   <zone id="AS0" routing="Full">
     <host id="host0" speed="1Gf"/>
     <host id="host1" speed="2Gf"/>
     <host id="host2" speed="40Gf"/>
     <link id="link0" bandwidth="125MBps" latency="100us"/>
     <link id="link1" bandwidth="50MBps" latency="150us"/>
     <link id="link2" bandwidth="250MBps" latency="50us"/>
     <route src="host0" dst="host1"><link_ctn id="link0"/><link_ctn id="link1"/></route>
     <route src="host1" dst="host2"><link_ctn id="link1"/><link_ctn id="link2"/></route>
    <route src="host0" dst="host2"><link_ctn id="link0"/><link_ctn id="link2"/></route>
  </zone>
</platform>
    
    #+end_src

*** /Cluster/ homogêneo com um /crossbar switch/

    Uma plataforma de processamento paralelo muito comum é um /cluster/
    homogêneo, contendo N /hosts/ conectados em um /switch/ que é muito
    mais rápido que as conexões, portanto sua latência e banda são
    desconsideradas.

    #+CAPTION: Figura que demonstra a topologia com um crossbar switch.
    #+NAME: fig:CROSSBAR

    [[./img/crossbar.png]]


    #+begin_src xml
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
<platform version="4.1">
    <zone id="AS0" routing="Full">
        <cluster id="my_cluster" prefix="host-" suffix=".hawaii.edu" radical="0-255" speed="1Gf" bw="125Mbps" lat="5us"/>
    </zone>
</platform>
    
    #+end_src

*** /Cluster/ homogêneo com um /backbone/ compartilhado
    
    Uma plataforma de processamento paralelo muito comum é um /cluster/
    homogêneo conectado a um meio de comunicação compartilhado, um
    /backbone/, que contém capacidade de banda finita.

    #+CAPTION: Figura que demonstra a topologia com um backbone shared.
    #+NAME: fig:BACKBONE

    [[./img/backbone.png]]

    #+begin_src xml
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
<platform version="4.1">
  <zone id="AS0" routing="Full">
    <cluster id="my_cluster" prefix="host−" suffix=".hawaii.edu" radical="0−255" speed="1Gf" bw="125Mbps" lat="50us" bb_bw="2.25Gbps" bb_lat="500us"/>
  </zone>
</platform>

    #+end_src

*** Dois /clusters/ conectados

    É possível conectar /clusters/ e de fato construir ambientes de
    procesamento paralelo com hierarquias. 


    #+CAPTION: Figura que demonstra a topologia com dois clusters conectados.
    #+NAME: fig:2-CLUSTERS
    [[./img/2-clusters.png]]

    #+begin_src xml
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
<platform version="4.1">
  <zone id="AS0" routing="Full">
    <cluster id="my_cluster_1" prefix="C1−" suffix=".hawaii.edu" radical="0−15" speed="1Gf" bw="125Mbps" lat="50us" bb_bw="2.25Gbps" bb_lat="500us" />
    <cluster id="my_cluster_2" prefix="C2−" suffix=".hawaii.edu" radical="0−31" speed="2Gf" bw="125Mbps" lat="50us" />
    <link id="internet_backbone" bandwidth="0.01Gbps" latency="22500us" />
    <zoneRoute src="my_cluster_1" dst="my_cluster_2" gw_src="C1−my_cluster_1_router.hawaii.edu" gw_dst="C2−my_cluster_2_router.hawaii.edu" symmetrical="YES">
      <link_ctn id="internet_backbone" />
    </zoneRoute>
  </zone>
</platform>
    
    #+end_src
** Primeiro programa usando SMPI
 
   *Contextualização :* Consiste em um programa onde os processos passam uma mensagem
   adiante em uma topologia de anel, e após todas mensagens serem
   enviadas e o nodo #0 receber a mensagem, ele imprime o
   tempo gasto na tela.

*** Código em C

   #+begin_src C

#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

#define N (1024 * 1024 * 1)

int main(int argc, char *argv[])
{
  int size, rank;
  struct timeval start, end;
  char hostname[256];
  int hostname_len;

  MPI_Init(&argc, &argv);

  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  MPI_Get_processor_name(hostname,&hostname_len);

  // Allocate a 10MiB buffer
  char *buffer = malloc(sizeof(char) * N);

  // Communicate along the ring
  if (rank == 0) {
        gettimeofday(&start,NULL);
        printf("Rank %d (running on '%s'): sending the message rank %d\n",rank,hostname,1);
	MPI_Send(buffer, N, MPI_BYTE, 1, 1, MPI_COMM_WORLD);
       	MPI_Recv(buffer, N, MPI_BYTE, size-1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Rank %d (running on '%s'): received the message from rank %d\n",rank,hostname,size-1);
  	gettimeofday(&end,NULL);
  	printf("%f\n",(end.tv_sec*1000000.0 + end.tv_usec -
		 	start.tv_sec*1000000.0 - start.tv_usec) / 1000000.0);

  } else {
       	MPI_Recv(buffer, N, MPI_BYTE, rank-1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        printf("Rank %d (running on '%s'): receive the message and sending it to rank %d\n",rank,hostname,(rank+1)%size);
	MPI_Send(buffer, N, MPI_BYTE, (rank+1)%size, 1, MPI_COMM_WORLD);
  }

  MPI_Finalize();
  return 0;
}

#+end_src

   Para simular esse código utilizando o *SIMGRID* é necessário mais
   dois arquivos: o arquivo de configuração da plataforma e o arquivo
   de configuração dos /hosts/, que basicamente lista todos os hosts
   disponíveis.

*** Configuração da plataforma

    #+begin_src xml
    <?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
<platform version="4.1">
    <zone id="AS0" routing="Full">
        <cluster id="my_cluster" prefix="host-" suffix=".hawaii.edu" radical="0-255" speed="1Gf" bw="125Mbps" lat="5us"/>
    </zone>
</platform>

    #+end_src
*** Lista de hosts

    #+begin_src txt
host-0.hawaii.edu
host-1.hawaii.edu
host-2.hawaii.edu
host-3.hawaii.edu
host-4.hawaii.edu
host-5.hawaii.edu
host-6.hawaii.edu
host-7.hawaii.edu
host-8.hawaii.edu
host-9.hawaii.edu
host-10.hawaii.edu
host-11.hawaii.edu
host-12.hawaii.edu
host-13.hawaii.edu
host-14.hawaii.edu
host-15.hawaii.edu
host-16.hawaii.edu
host-17.hawaii.edu
host-18.hawaii.edu
host-19.hawaii.edu
host-20.hawaii.edu
host-21.hawaii.edu
host-22.hawaii.edu
host-23.hawaii.edu
host-24.hawaii.edu
host-25.hawaii.edu
host-26.hawaii.edu
host-27.hawaii.edu
host-28.hawaii.edu
host-29.hawaii.edu
host-30.hawaii.edu
host-31.hawaii.edu
host-32.hawaii.edu
host-33.hawaii.edu
host-34.hawaii.edu
host-35.hawaii.edu
host-36.hawaii.edu
host-37.hawaii.edu
host-38.hawaii.edu
host-39.hawaii.edu
host-40.hawaii.edu
host-41.hawaii.edu
host-42.hawaii.edu
host-43.hawaii.edu
host-44.hawaii.edu
host-45.hawaii.edu
host-46.hawaii.edu
host-47.hawaii.edu
host-48.hawaii.edu
host-49.hawaii.edu
host-50.hawaii.edu
host-51.hawaii.edu
host-52.hawaii.edu
host-53.hawaii.edu
host-54.hawaii.edu
host-55.hawaii.edu
host-56.hawaii.edu
host-57.hawaii.edu
host-58.hawaii.edu
host-59.hawaii.edu
host-60.hawaii.edu
host-61.hawaii.edu
host-62.hawaii.edu
host-63.hawaii.edu
host-64.hawaii.edu
host-65.hawaii.edu
host-66.hawaii.edu
host-67.hawaii.edu
host-68.hawaii.edu
host-69.hawaii.edu
host-70.hawaii.edu
host-71.hawaii.edu
host-72.hawaii.edu
host-73.hawaii.edu
host-74.hawaii.edu
host-75.hawaii.edu
host-76.hawaii.edu
host-77.hawaii.edu
host-78.hawaii.edu
host-79.hawaii.edu
host-80.hawaii.edu
host-81.hawaii.edu
host-82.hawaii.edu
host-83.hawaii.edu
host-84.hawaii.edu
host-85.hawaii.edu
host-86.hawaii.edu
host-87.hawaii.edu
host-88.hawaii.edu
host-89.hawaii.edu
host-90.hawaii.edu
host-91.hawaii.edu
host-92.hawaii.edu
host-93.hawaii.edu
host-94.hawaii.edu
host-95.hawaii.edu
host-96.hawaii.edu
host-97.hawaii.edu
host-98.hawaii.edu
host-99.hawaii.edu
host-100.hawaii.edu
host-101.hawaii.edu
host-102.hawaii.edu
host-103.hawaii.edu
host-104.hawaii.edu
host-105.hawaii.edu
host-106.hawaii.edu
host-107.hawaii.edu
host-108.hawaii.edu
host-109.hawaii.edu
host-110.hawaii.edu
host-111.hawaii.edu
host-112.hawaii.edu
host-113.hawaii.edu
host-114.hawaii.edu
host-115.hawaii.edu
host-116.hawaii.edu
host-117.hawaii.edu
host-118.hawaii.edu
host-119.hawaii.edu
host-120.hawaii.edu
host-121.hawaii.edu
host-122.hawaii.edu
host-123.hawaii.edu
host-124.hawaii.edu
host-125.hawaii.edu
host-126.hawaii.edu
host-127.hawaii.edu
host-128.hawaii.edu
host-129.hawaii.edu
host-130.hawaii.edu
host-131.hawaii.edu
host-132.hawaii.edu
host-133.hawaii.edu
host-134.hawaii.edu
host-135.hawaii.edu
host-136.hawaii.edu
host-137.hawaii.edu
host-138.hawaii.edu
host-139.hawaii.edu
host-140.hawaii.edu
host-141.hawaii.edu
host-142.hawaii.edu
host-143.hawaii.edu
host-144.hawaii.edu
host-145.hawaii.edu
host-146.hawaii.edu
host-147.hawaii.edu
host-148.hawaii.edu
host-149.hawaii.edu
host-150.hawaii.edu
host-151.hawaii.edu
host-152.hawaii.edu
host-153.hawaii.edu
host-154.hawaii.edu
host-155.hawaii.edu
host-156.hawaii.edu
host-157.hawaii.edu
host-158.hawaii.edu
host-159.hawaii.edu
host-160.hawaii.edu
host-161.hawaii.edu
host-162.hawaii.edu
host-163.hawaii.edu
host-164.hawaii.edu
host-165.hawaii.edu
host-166.hawaii.edu
host-167.hawaii.edu
host-168.hawaii.edu
host-169.hawaii.edu
host-170.hawaii.edu
host-171.hawaii.edu
host-172.hawaii.edu
host-173.hawaii.edu
host-174.hawaii.edu
host-175.hawaii.edu
host-176.hawaii.edu
host-177.hawaii.edu
host-178.hawaii.edu
host-179.hawaii.edu
host-180.hawaii.edu
host-181.hawaii.edu
host-182.hawaii.edu
host-183.hawaii.edu
host-184.hawaii.edu
host-185.hawaii.edu
host-186.hawaii.edu
host-187.hawaii.edu
host-188.hawaii.edu
host-189.hawaii.edu
host-190.hawaii.edu
host-191.hawaii.edu
host-192.hawaii.edu
host-193.hawaii.edu
host-194.hawaii.edu
host-195.hawaii.edu
host-196.hawaii.edu
host-197.hawaii.edu
host-198.hawaii.edu
host-199.hawaii.edu
host-200.hawaii.edu
host-201.hawaii.edu
host-202.hawaii.edu
host-203.hawaii.edu
host-204.hawaii.edu
host-205.hawaii.edu
host-206.hawaii.edu
host-207.hawaii.edu
host-208.hawaii.edu
host-209.hawaii.edu
host-210.hawaii.edu
host-211.hawaii.edu
host-212.hawaii.edu
host-213.hawaii.edu
host-214.hawaii.edu
host-215.hawaii.edu
host-216.hawaii.edu
host-217.hawaii.edu
host-218.hawaii.edu
host-219.hawaii.edu
host-220.hawaii.edu
host-221.hawaii.edu
host-222.hawaii.edu
host-223.hawaii.edu
host-224.hawaii.edu
host-225.hawaii.edu
host-226.hawaii.edu
host-227.hawaii.edu
host-228.hawaii.edu
host-229.hawaii.edu
host-230.hawaii.edu
host-231.hawaii.edu
host-232.hawaii.edu
host-233.hawaii.edu
host-234.hawaii.edu
host-235.hawaii.edu
host-236.hawaii.edu
host-237.hawaii.edu
host-238.hawaii.edu
host-239.hawaii.edu
host-240.hawaii.edu
host-241.hawaii.edu
host-242.hawaii.edu
host-243.hawaii.edu
host-244.hawaii.edu
host-245.hawaii.edu
host-246.hawaii.edu
host-247.hawaii.edu
host-248.hawaii.edu
host-249.hawaii.edu
host-250.hawaii.edu
host-251.hawaii.edu
host-252.hawaii.edu
host-253.hawaii.edu
host-254.hawaii.edu
host-255.hawaii.edu    

    #+end_src
    
*** Execução do programa

   Após a criação desse arquvio, basta *compilar e executar* o código.

   #+begin_src shell
   smpicc -O4 roundtrip.c -o roundtrip
   #+end_src

   #+begin_src shell
   smpirun -np 16 -hostfile ./cluster_hostfile.txt -platform ./cluster_crossbar.xml ./roundtrip
   #+end_src

   - A flag *-np* serve para explicitar o número de processos MPI que
   serão utilizados.
   - A flag *-hostfile* serve para adicionar a listagem de /hosts/
     disponíveis.
   - A flag *-plataform* serve para explicitar a configuração da
     plataforma que será usada no experimento.
